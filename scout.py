import os
import json
import asyncio
import requests
import html
import re
import logging
from datetime import datetime, timedelta, timezone
from aiogram import Bot
from aiogram.client.default import DefaultBotProperties
from aiogram.enums import ParseMode
from groq import Groq

# ============ LOGGING ============

logging.basicConfig(
    level=logging.DEBUG,  # ‚Üê –ò–ó–ú–ï–ù–ï–ù–û: DEBUG –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
    format='%(asctime)s | %(levelname)s | %(message)s',
    handlers=[
        logging.FileHandler('scout_radar.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ============ CONFIG ============

GROQ_API_KEY = os.getenv("GROQ_API_KEY")
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
TARGET_CHANNEL_ID = os.getenv("CHANNEL_ID")
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")

STATE_FILE = "scout_history.json"

MAX_AGE_DAYS = 3
MAX_POSTS_PER_RUN = 100
GROQ_DELAY = 2
MESSAGE_DELAY = 3
MIN_STARS = 0
MIN_API_CALLS_REMAINING = 50

API_HEADERS = {
    "Authorization": f"Bearer {GITHUB_TOKEN}",
    "Accept": "application/vnd.github.v3+json"
}

bot = Bot(token=TELEGRAM_BOT_TOKEN, default=DefaultBotProperties(parse_mode=ParseMode.HTML))
groq_client = Groq(api_key=GROQ_API_KEY)

# ============ –ê–ì–†–ï–ì–ê–¢–û–†–´ ============
KNOWN_AGGREGATORS = [
    {"owner": "Leon406", "repo": "SubCrawler", "name": "SubCrawler"},
    {"owner": "peasoft", "repo": "NoMoreWalls", "name": "NoMoreWalls"},
    {"owner": "barry-far", "repo": "V2ray-Configs", "name": "V2ray-Configs"},
]

# ============ –ü–û–ò–°–ö–û–í–´–ï –ó–ê–ü–†–û–°–´ ============
FRESH_SEARCHES = [
    {"name": "Roskomsvoboda", "title": "üì¢ –†–æ—Å–∫–æ–º—Å–≤–æ–±–æ–¥–∞ / RuBlacklist", "query": "roskomsvoboda OR rublacklist OR runet-censorship", "priority": 10},
    {"name": "RKN & TSPU", "title": "üëÅ –†–ö–ù & –¢–°–ü–£", "query": "roskomnadzor OR rkn OR tspu OR sorm", "priority": 10},
    {"name": "Blocklist RU", "title": "‚õîÔ∏è –†–µ–µ—Å—Ç—Ä—ã –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫", "query": "russia blocklist OR reestr-zapret OR zapret-info", "priority": 9},
    {"name": "AntiZapret", "title": "üõ° AntiZapret", "query": "antizapret OR anti-zapret", "priority": 10},
    {"name": "Zapret", "title": "üõ† Zapret DPI", "query": "zapret dpi OR zapret-discord OR zapret-winws", "priority": 9},
    {"name": "ByeDPI", "title": "üõ† ByeDPI / GoodbyeDPI", "query": "byedpi OR goodbyedpi", "priority": 9},
    {"name": "SpoofDPI", "title": "üõ† SpoofDPI", "query": "spoofdpi OR dpi-tunnel", "priority": 8},
    {"name": "VLESS RU", "title": "üîß VLESS Russia", "query": "vless russia OR vless reality", "priority": 8},
    {"name": "Hysteria2", "title": "üöÄ Hysteria 2", "query": "hysteria2 config OR hysteria2-server", "priority": 8},
    {"name": "Amnezia", "title": "üõ° Amnezia VPN", "query": "amnezia vpn OR amneziawg", "priority": 9},
    {"name": "Shadowsocks", "title": "üîê Shadowsocks 2022", "query": "shadowsocks-2022 OR ss2022", "priority": 7},
    {"name": "Marzban", "title": "üéõ Marzban", "query": "marzban panel OR marzban-node", "priority": 8},
    {"name": "3X-UI", "title": "üéõ 3X-UI / X-UI", "query": "3x-ui OR x-ui panel", "priority": 7},
    {"name": "Geosite RU", "title": "üó∫ Geosite / GeoIP RU", "query": "geosite russia OR geoip russia", "priority": 7},
    {"name": "Whitelist RU", "title": "‚úÖ –ë–µ–ª—ã–µ —Å–ø–∏—Å–∫–∏ –†–§", "query": "russia whitelist OR russian-whitelist OR domestic-whitelist OR gosuslugi-whitelist", "priority": 10},
    {"name": "NoDPI", "title": "üõ† NoDPI", "query": "nodpi python OR dpi-bypass-python", "priority": 8},
    {"name": "Cloak", "title": "üé≠ Cloak", "query": "cloak censorship OR cbeuw-cloak", "priority": 8},
    {"name": "TrustTunnel", "title": "üîí TrustTunnel", "query": "trusttunnel OR adguard-vpn-protocol", "priority": 8},
    {"name": "Trojan-Go", "title": "üê¥ Trojan-Go", "query": "trojan-go russia OR trojan-gfw", "priority": 7},
    {"name": "Outline VPN", "title": "üì° Outline VPN", "query": "outline vpn OR outline-server russia", "priority": 8},
    {"name": "Hiddify", "title": "üéõ Hiddify Manager", "query": "hiddify manager OR hiddify-next", "priority": 8},
    {"name": "V2Board", "title": "üéõ V2Board", "query": "v2board russia OR v2ray-panel", "priority": 7},
    {"name": "Domain Lists", "title": "üìã –°–ø–∏—Å–∫–∏ –¥–æ–º–µ–Ω–æ–≤ –†–§", "query": "russia domain-list OR ru-domain-routing", "priority": 8},
    {"name": "IP Lists RU", "title": "üåê IP —Å–ø–∏—Å–∫–∏ –†–§", "query": "russia ip-list OR russian-networks OR ru-cidr", "priority": 7},
    {"name": "Routing Rules", "title": "üß∂ –ü—Ä–∞–≤–∏–ª–∞ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏", "query": "russia routing-rules OR split-routing russia", "priority": 8},
    {"name": "Nekoray", "title": "üê± Nekoray / V2RayN", "query": "nekoray OR v2rayn russia", "priority": 7},
    {"name": "Clash Meta", "title": "‚öîÔ∏è Clash Meta", "query": "clash-meta russia OR clash-verge", "priority": 7},
    {"name": "Sing-Box", "title": "üì¶ Sing-Box", "query": "sing-box russia OR sing-box-subscribe", "priority": 8},
    {"name": "BypassHub", "title": "üîó BypassHub", "query": "bypasshub OR censorship-abstraction", "priority": 7},
    {"name": "SNI Proxy", "title": "üéè SNI Proxy", "query": "sni-proxy russia OR sni-routing", "priority": 7},
    {"name": "XTLS Reality", "title": "üåú XTLS Reality", "query": "xtls-reality OR reality-protocol", "priority": 8},
    {"name": "Obfuscation", "title": "üå• –û–±—Ñ—É—Å–∫–∞—Ü–∏—è —Ç—Ä–∞—Ñ–∏–∫–∞", "query": "traffic-obfuscation russia OR vpn-obfuscation", "priority": 7},
    {"name": "CDN Fronting", "title": "‚òÅÔ∏è CDN Fronting", "query": "cdn-fronting russia OR domain-fronting cloudflare", "priority": 7},
    {"name": "DNS-over-HTTPS", "title": "üîê DNS-over-HTTPS", "query": "doh russia OR dns-over-https bypass", "priority": 7},
    {"name": "DNS-over-TLS", "title": "üîê DNS-over-TLS", "query": "dot russia OR dns-over-tls", "priority": 7},
    {"name": "Encrypted SNI", "title": "üîí Encrypted SNI", "query": "esni russia OR encrypted-client-hello", "priority": 7},
    {"name": "Config Generators", "title": "‚öôÔ∏è –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä—ã –∫–æ–Ω—Ñ–∏–≥–æ–≤", "query": "v2ray-config-generator russia OR subscription-converter", "priority": 7},
    {"name": "Auto Subscribe", "title": "üì° –ê–≤—Ç–æ–ø–æ–¥–ø–∏—Å–∫–∏", "query": "v2ray-subscription OR proxy-subscription russia", "priority": 6},
    {"name": "Speed Test", "title": "‚ö°Ô∏è –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ VPN", "query": "vpn-speed-test russia OR proxy-checker", "priority": 6},
]

FRESH_SEARCHES.sort(key=lambda x: x.get('priority', 5), reverse=True)

# ============ VALIDATION ============

def validate_env():
    required = {
        "GROQ_API_KEY": GROQ_API_KEY,
        "TELEGRAM_BOT_TOKEN": TELEGRAM_BOT_TOKEN,
        "CHANNEL_ID": TARGET_CHANNEL_ID,
        "GITHUB_TOKEN": GITHUB_TOKEN
    }
    
    missing = [k for k, v in required.items() if not v]
    
    if missing:
        logger.error(f"‚ùå Missing environment variables: {', '.join(missing)}")
        return False
    
    logger.info("‚úÖ All environment variables validated")
    return True

# ============ GITHUB API RATE LIMIT ============

def check_rate_limit():
    try:
        resp = requests.get("https://api.github.com/rate_limit", headers=API_HEADERS, timeout=10)
        if resp.status_code == 200:
            data = resp.json()
            remaining = data['rate']['remaining']
            limit = data['rate']['limit']
            reset_time = datetime.fromtimestamp(data['rate']['reset'], timezone.utc)
            
            logger.info(f"üìä GitHub API: {remaining}/{limit} calls remaining")
            
            if remaining < MIN_API_CALLS_REMAINING:
                logger.warning(f"‚ö†Ô∏è API limit low ({remaining} left). Reset at {reset_time.strftime('%H:%M:%S UTC')}")
                
                if remaining < 10:
                    logger.error(f"‚è∏ Critical: Only {remaining} calls left. Stopping.")
                    return False
            
            return True
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Could not check rate limit: {e}")
        return True

# ============ HELPERS ============

def has_non_latin(text):
    if not text: 
        return False
    
    patterns = [
        r'[\u4e00-\u9fff\u3040-\u309f\u30a0-\u30ff\uac00-\ud7af]',
        r'[\u0600-\u06ff\u0750-\u077f\uFB50-\uFDFF\uFE70-\uFEFF]',
        r'[\u0e00-\u0e7f\u1780-\u17ff]',
    ]
    
    return any(re.search(p, text) for p in patterns)

def is_repo_empty(owner, repo, cache):
    """‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –£–º–µ–Ω—å—à–µ–Ω TTL –∫—ç—à–∞ –¥–æ 6 —á–∞—Å–æ–≤"""
    key = f"{owner}/{repo}"
    
    if key in cache:
        try:
            cached_time = datetime.fromisoformat(cache[key]['checked_at'])
            # ‚úÖ –ò–ó–ú–ï–ù–ï–ù–û: 6 —á–∞—Å–æ–≤ –≤–º–µ—Å—Ç–æ 24
            if (datetime.now(timezone.utc) - cached_time).total_seconds() < 21600:
                logger.debug(f"   üì¶ Cache hit for {key}: empty={cache[key]['is_empty']}")
                return cache[key]['is_empty']
        except:
            pass
    
    try:
        url = f"https://api.github.com/repos/{owner}/{repo}"
        resp = requests.get(url, headers=API_HEADERS, timeout=10)
        
        if resp.status_code != 200:
            logger.debug(f"   ‚ö†Ô∏è Repo check failed for {key}: status {resp.status_code}")
            result = True
        else:
            data = resp.json()
            size = data.get('size', 0)
            issues = data.get('open_issues_count', 0)
            stars = data.get('stargazers_count', 0)
            
            result = size < 5 or (issues == 0 and stars == 0 and size < 50)
            logger.debug(f"   üì¶ Repo {key}: size={size}, stars={stars}, issues={issues}, empty={result}")
        
        cache[key] = {
            'is_empty': result,
            'checked_at': datetime.now(timezone.utc).isoformat()
        }
        
        return result
    except Exception as e:
        logger.debug(f"Error checking {key}: {e}")
        return False  # ‚úÖ –ò–ó–ú–ï–ù–ï–ù–û: –ü—Ä–∏ –æ—à–∏–±–∫–µ –ù–ï —Å—á–∏—Ç–∞–µ–º –ø—É—Å—Ç—ã–º

def is_likely_fork_spam(item):
    if not item.get('fork'):
        return False
    
    if item.get('stargazers_count', 0) == 0 and item.get('forks_count', 0) == 0:
        return True
    
    created = item.get('created_at')
    pushed = item.get('pushed_at')
    if created and pushed:
        try:
            created_dt = datetime.fromisoformat(created.replace('Z', '+00:00'))
            pushed_dt = datetime.fromisoformat(pushed.replace('Z', '+00:00'))
            if abs((pushed_dt - created_dt).total_seconds()) < 60:
                return True
        except:
            pass
    
    return False

def safe_desc(desc, max_len=120):
    if desc is None:
        return ""
    
    desc = str(desc).strip()
    desc = re.sub(r'[üî•‚ö°Ô∏è‚ú®üéâ]{3,}', '', desc)
    
    return desc[:max_len] if desc else ""

def get_age_hours(date_string):
    try:
        if not date_string: 
            return 9999
        dt = datetime.fromisoformat(date_string.replace('Z', '+00:00'))
        return (datetime.now(timezone.utc) - dt).total_seconds() / 3600
    except: 
        return 9999

def get_freshness(date_string):
    hours = get_age_hours(date_string)
    if hours < 1: return "üî• –¢–æ–ª—å–∫–æ —á—Ç–æ"
    elif hours < 6: return f"üî• {int(hours)}—á –Ω–∞–∑–∞–¥"
    elif hours < 24: return "üî• –°–µ–≥–æ–¥–Ω—è"
    elif hours < 48: return "‚úÖ –í—á–µ—Ä–∞"
    else: return f"üìÖ {int(hours/24)}–¥ –Ω–∞–∑–∞–¥"

def is_fresh(date_string):
    """‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –î–æ–±–∞–≤–ª–µ–Ω–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ"""
    hours = get_age_hours(date_string)
    max_hours = MAX_AGE_DAYS * 24
    is_ok = hours <= max_hours
    if not is_ok:
        logger.debug(f"   ‚è∞ Not fresh: {hours:.1f}h > {max_hours}h limit")
    return is_ok

def quick_filter(name, desc, stars=0):
    """‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –î–æ–±–∞–≤–ª–µ–Ω–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏—á–∏–Ω –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è"""
    text = f"{name} {desc or ''}".lower()
    full_text = f"{name} {desc or ''}"

    if has_non_latin(full_text):
        logger.debug(f"   ‚ùå FILTER: hieroglyphs in {name}")
        return False

    if stars < MIN_STARS:
        logger.debug(f"   ‚ùå FILTER: stars={stars} < {MIN_STARS} for {name}")
        return False

    whitelist = [
        'russia', 'russian', 'ru-block', 'roskomnadzor', 'rkn', 'antizapret',
        'zapret', 'mintsifry', 'tspu', 'sorm', '—Ä–æ—Å–∫–æ–º–Ω–∞–¥–∑–æ—Ä', '—Ä—Ñ',
        'amnezia', 'hysteria', 'reality', 'marzban', 'xray-core',
        'v2ray', 'vless', 'trojan', 'shadowsocks', 'clash', 'sing-box',
        'bypass', 'proxy', 'vpn', 'dpi', 'gfw'  # ‚úÖ –î–û–ë–ê–í–õ–ï–ù–û: –±–æ–ª—å—à–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    ]
    if any(w in text for w in whitelist):
        logger.debug(f"   ‚úÖ FILTER: whitelist match for {name}")
        return True

    blacklist = [
        'china', 'chinese', 'cn-', 'iran', 'persian', 'vietnam',
        'homework', 'tutorial', 'example-', 'template', 'deprecated',
        'test-repo', 'demo-', 'practice', 'learning'
    ]
    for kw in blacklist:
        if kw in text:
            logger.debug(f"   ‚ùå FILTER: blacklist '{kw}' in {name}")
            return False

    noise_patterns = [
        r'\d{4,}',
        r'[A-Z]{8,}',
        r'[-_]{3,}',
    ]
    for p in noise_patterns:
        if re.search(p, name):
            logger.debug(f"   ‚ùå FILTER: noise pattern in {name}")
            return False

    # ‚úÖ –ò–ó–ú–ï–ù–ï–ù–û: –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –ü–†–û–ü–£–°–ö–ê–ï–ú, –∞ –Ω–µ –±–ª–æ–∫–∏—Ä—É–µ–º
    logger.debug(f"   ‚ö†Ô∏è FILTER: no match, allowing {name}")
    return True

def build_post(title, repo_full_name, stars, freshness, description, url):
    return (
        f"<b>{title}</b>\n\n"
        f"üì¶ <code>{html.escape(repo_full_name)}</code>\n"
        f"‚≠êÔ∏è {stars} | ‚è∞ {freshness}\n"
        f"üí° {html.escape(description)}\n\n"
        f"üîó <a href='{url}'>–û—Ç–∫—Ä—ã—Ç—å –Ω–∞ GitHub</a>"
    )

def load_state():
    if os.path.exists(STATE_FILE):
        try:
            with open(STATE_FILE, "r", encoding='utf-8') as f:
                data = json.load(f)
                logger.info(f"üìÇ Loaded state: {len(data.get('posted', []))} posted, {len(data.get('commits', {}))} commits tracked")
                return data
        except Exception as e:
            logger.warning(f"Could not load state: {e}")
    return {"posted": [], "commits": {}, "repo_cache": {}, "last_run": None}

def save_state(state):
    state['last_run'] = datetime.now(timezone.utc).isoformat()
    try:
        with open(STATE_FILE, "w", encoding='utf-8') as f:
            json.dump(state, f, indent=2, ensure_ascii=False)
        logger.info(f"üíæ State saved ({len(state['posted'])} posted repos)")
    except Exception as e:
        logger.error(f"‚ùå Could not save state: {e}")

def get_last_commit(owner, repo):
    """‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–æ–º–º–∏—Ç–æ–≤"""
    url = f"https://api.github.com/repos/{owner}/{repo}/commits?per_page=5"
    try:
        resp = requests.get(url, headers=API_HEADERS, timeout=10)
        if resp.status_code == 200 and resp.json():
            for c in resp.json():
                msg = c['commit']['message'].split('\n')[0][:60]
                
                if has_non_latin(msg):
                    logger.debug(f"   ‚è≠ SKIP commit (hieroglyphs): {owner}/{repo}")
                    continue
                
                commit_date = c['commit']['committer']['date']
                
                # ‚úÖ –î–û–ë–ê–í–õ–ï–ù–û: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–≤–µ–∂–µ—Å—Ç–∏ –∫–æ–º–º–∏—Ç–∞
                if not is_fresh(commit_date):
                    logger.debug(f"   ‚è≠ SKIP commit (old): {owner}/{repo} - {commit_date}")
                    continue
                
                return {
                    "sha": c['sha'][:7],
                    "date": commit_date,
                    "msg": msg,
                    "url": c['html_url']
                }
            
            logger.debug(f"   ‚ö†Ô∏è No valid fresh commits for {owner}/{repo}")
    except Exception as e:
        logger.debug(f"Error getting commit for {owner}/{repo}: {e}")
    return None

def get_recent_commits(owner, repo, since_sha=None):
    """‚úÖ –ù–û–í–ê–Ø –§–£–ù–ö–¶–ò–Ø: –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –Ω–æ–≤—ã—Ö –∫–æ–º–º–∏—Ç–æ–≤ –ø–æ—Å–ª–µ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–≥–æ SHA"""
    url = f"https://api.github.com/repos/{owner}/{repo}/commits?per_page=20"
    try:
        resp = requests.get(url, headers=API_HEADERS, timeout=10)
        if resp.status_code != 200:
            return []
        
        commits = []
        for c in resp.json():
            sha = c['sha'][:7]
            
            if since_sha and sha == since_sha:
                break
            
            msg = c['commit']['message'].split('\n')[0][:60]
            
            if has_non_latin(msg):
                continue
            
            commit_date = c['commit']['committer']['date']
            if not is_fresh(commit_date):
                break
            
            commits.append({
                "sha": sha,
                "date": commit_date,
                "msg": msg,
                "url": c['html_url']
            })
        
        return commits
    except Exception as e:
        logger.debug(f"Error getting commits for {owner}/{repo}: {e}")
    return []

def search_fresh_repos(query, per_page=50):  # ‚úÖ –ò–ó–ú–ï–ù–ï–ù–û: 50 –≤–º–µ—Å—Ç–æ 30
    """‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏"""
    results = []
    
    # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: pushed:>date
    date_filter = (datetime.now(timezone.utc) - timedelta(days=MAX_AGE_DAYS)).strftime('%Y-%m-%d')
    
    strategies = [
        f"{query}+pushed:>{date_filter}",
        f"{query}+created:>{date_filter}",  # ‚úÖ –î–û–ë–ê–í–õ–ï–ù–û: –Ω–æ–≤—ã–µ —Ä–µ–ø–æ
    ]
    
    seen_ids = set()
    
    for strategy in strategies:
        url = (
            f"https://api.github.com/search/repositories"
            f"?q={strategy}"
            f"&sort=updated&order=desc&per_page={per_page}"
        )
        
        try:
            resp = requests.get(url, headers=API_HEADERS, timeout=15)
            
            if resp.status_code == 200:
                items = resp.json().get('items', [])
                logger.debug(f"   üîç Strategy '{strategy[:50]}...': found {len(items)} repos")
                
                for item in items:
                    if item['id'] not in seen_ids:
                        seen_ids.add(item['id'])
                        
                        # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü—Ä–æ–≤–µ—Ä—è–µ–º –ò pushed_at –ò updated_at
                        pushed_at = item.get('pushed_at')
                        updated_at = item.get('updated_at')
                        
                        if is_fresh(pushed_at) or is_fresh(updated_at):
                            results.append(item)
                        else:
                            logger.debug(f"   ‚è∞ Skip {item['full_name']}: pushed={pushed_at}, updated={updated_at}")
                            
            elif resp.status_code == 403:
                logger.warning("‚ö†Ô∏è GitHub API rate limit hit!")
                break
            else:
                logger.warning(f"‚ö†Ô∏è Search failed with status {resp.status_code}")
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Search error: {e}")
    
    logger.info(f"   üìä Total unique fresh repos found: {len(results)}")
    return results

def check_repo_activity(owner, repo):
    """‚úÖ –ù–û–í–ê–Ø –§–£–ù–ö–¶–ò–Ø: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∞–ª—å–Ω–æ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è"""
    url = f"https://api.github.com/repos/{owner}/{repo}/events?per_page=10"
    try:
        resp = requests.get(url, headers=API_HEADERS, timeout=10)
        if resp.status_code == 200:
            events = resp.json()
            for event in events:
                event_date = event.get('created_at')
                if event_date and is_fresh(event_date):
                    event_type = event.get('type', 'Unknown')
                    logger.debug(f"   ‚úÖ Fresh activity: {event_type} at {event_date}")
                    return True
            logger.debug(f"   ‚ö†Ô∏è No fresh events for {owner}/{repo}")
        return False
    except Exception as e:
        logger.debug(f"Error checking activity for {owner}/{repo}: {e}")
        return False

async def analyze_relevance(repos):
    if not repos: 
        return {}

    text = "\n".join([
        f"{i+1}. {r['full_name']} | ‚≠ê{r['stargazers_count']} | {safe_desc(r['description'], 80)}" 
        for i, r in enumerate(repos)
    ])

    prompt = f"""–ó–∞–¥–∞—á–∞: –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å GitHub —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ –¥–ª—è –∫–∞–Ω–∞–ª–∞ –ø—Ä–æ –æ–±—Ö–æ–¥ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫ –≤ –†–§.

–¶–µ–ª–µ–≤–∞—è —Ç–µ–º–∞:
- VPN, –ø—Ä–æ–∫—Å–∏, DPI-–æ–±—Ö–æ–¥ (Zapret, ByeDPI, AntiZapret, Amnezia)
- –¶–µ–Ω–∑—É—Ä–∞ –≤ –†–§ (–†–ö–ù, –¢–°–ü–£, –ú–∏–Ω—Ü–∏—Ñ—Ä—ã, –†–æ—Å–∫–æ–º–Ω–∞–¥–∑–æ—Ä)
- –ü–æ–ª–µ–∑–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥–∏, —Å–ø–∏—Å–∫–∏ IP/–¥–æ–º–µ–Ω–æ–≤ –¥–ª—è –†–æ—Å—Å–∏–∏ –∏ –ï–≤—Ä–æ–ø—ã
- –ü–∞–Ω–µ–ª–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è (Marzban, 3X-UI, Hiddify)
- –ü—Ä–æ—Ç–æ–∫–æ–ª—ã: VLESS, Hysteria, Trojan, Shadowsocks, WireGuard
- –ö–ª–∏–µ–Ω—Ç—ã: Nekoray, Clash, Sing-Box, V2RayN

–°–ø–∏—Å–æ–∫ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤:
{text}

–û—Ç–≤–µ—Ç—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ: GOOD –∏–ª–∏ SKIP

GOOD –µ—Å–ª–∏:
‚úÖ –†–µ–∞–ª—å–Ω–æ –ø–æ–ª–µ–∑–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç/–∫–æ–Ω—Ñ–∏–≥ –¥–ª—è –æ–±—Ö–æ–¥–∞ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫
‚úÖ –°–≤—è–∑–∞–Ω —Å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-—Ü–µ–Ω–∑—É—Ä–æ–π (–Ω–µ —Ç–æ–ª—å–∫–æ –†–§, –Ω–æ –∏ –ø–æ–ª–µ–∑–Ω—ã–π –¥–ª—è –†–§)
‚úÖ –ê–∫—Ç—É–∞–ª—å–Ω—ã–µ —Å–ø–∏—Å–∫–∏/–±–∞–∑—ã/–∫–æ–Ω—Ñ–∏–≥–∏
‚úÖ –§–æ—Ä–∫ —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ —É–ª—É—á—à–µ–Ω–∏—è–º–∏

SKIP –µ—Å–ª–∏:
‚ùå –£—á–µ–±–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã, –¥–æ–º–∞—à–∫–∞, —è–≤–Ω–æ —É—Å—Ç–∞—Ä–µ–≤—à–∏–π –ø—Ä–æ–µ–∫—Ç
‚ùå –ü—É—Å—Ç–æ–π —Ñ–æ—Ä–∫ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
‚ùå –ú—É—Å–æ—Ä, —Å–ø–∞–º, —Ä–µ–∫–ª–∞–º–∞
‚ùå –ù–µ —Å–≤—è–∑–∞–Ω —Å VPN/–ø—Ä–æ–∫—Å–∏/—Ü–µ–Ω–∑—É—Ä–æ–π –≤–æ–æ–±—â–µ

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ (–°–¢–†–û–ì–û):
1: GOOD
2: SKIP
..."""

    try:
        resp = groq_client.chat.completions.create(
            model="llama-3.1-8b-instant",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=300,
            temperature=0.1
        )
        
        response_text = resp.choices[0].message.content
        logger.debug(f"   ü§ñ AI response:\n{response_text}")
        
        res = {}
        for line in response_text.split('\n'):
            if ':' in line:
                try:
                    idx, verdict = line.split(':', 1)
                    idx = int(idx.strip().replace('.', ''))
                    is_good = 'GOOD' in verdict.upper()
                    res[idx] = is_good
                    logger.debug(f"   ü§ñ Repo #{idx}: {'GOOD' if is_good else 'SKIP'}")
                except: 
                    pass
        return res
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è AI error: {e}")
        # ‚úÖ –ò–ó–ú–ï–ù–ï–ù–û: –ü—Ä–∏ –æ—à–∏–±–∫–µ AI - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –≤—Å—ë (–∞ –Ω–µ –±–ª–æ–∫–∏—Ä—É–µ–º)
        return {i: True for i in range(1, len(repos) + 1)}

async def generate_desc(name, desc):
    if desc and len(desc) > 25 and not has_non_latin(desc): 
        return desc

    prompt = f"""–†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π: {name}
–¢–µ–∫—É—â–µ–µ –æ–ø–∏—Å–∞–Ω–∏–µ: {desc or '–æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç'}

–ó–∞–¥–∞—á–∞: –ù–∞–ø–∏—à–∏ –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ (1 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ, –¥–æ 100 —Å–∏–º–≤–æ–ª–æ–≤) –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.
–ö–æ–Ω—Ç–µ–∫—Å—Ç: VPN, –æ–±—Ö–æ–¥ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫, –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-—Ü–µ–Ω–∑—É—Ä–∞ –≤ –†–æ—Å—Å–∏–∏.
–í–ê–ñ–ù–û: –¢–æ–ª—å–∫–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º –∏–ª–∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º, –ë–ï–ó –∏–µ—Ä–æ–≥–ª–∏—Ñ–æ–≤!

–û–ø–∏—Å–∞–Ω–∏–µ:"""

    for attempt in range(2):
        try:
            resp = groq_client.chat.completions.create(
                model="llama-3.1-8b-instant",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=80,
                temperature=0.3
            )
            generated = resp.choices[0].message.content.strip()
            
            if generated and not has_non_latin(generated):
                return generated
                
        except Exception as e:
            logger.debug(f"AI description attempt {attempt+1} failed: {e}")
            await asyncio.sleep(1)
    
    return "–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –æ–±—Ö–æ–¥–∞ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫"

async def send_message_safe(chat_id, text):
    if has_non_latin(text):
        logger.warning("‚ö†Ô∏è Blocked message with hieroglyphs from sending!")
        return False
    
    for attempt in range(3):
        try:
            await bot.send_message(chat_id, text, disable_web_page_preview=True)
            return True
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Send attempt {attempt+1} failed: {e}")
            await asyncio.sleep(2 ** attempt)
    return False

async def main():
    logger.info("=" * 60)
    logger.info("üïµÔ∏è  SCOUT RADAR v7.1 (improved detection)")
    logger.info("=" * 60)

    if not validate_env():
        return

    if not check_rate_limit():
        logger.error("‚ùå Insufficient API calls. Exiting.")
        return

    state = load_state()
    posted = set(state.get("posted", []))  # ‚úÖ –ò–ó–ú–ï–ù–ï–ù–û: set –¥–ª—è O(1) lookup
    commits = state.get("commits", {})
    repo_cache = state.get("repo_cache", {})
    count = 0
    
    # ‚úÖ –î–û–ë–ê–í–õ–ï–ù–û: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    stats = {
        "total_found": 0,
        "skipped_posted": 0,
        "skipped_filter": 0,
        "skipped_fork": 0,
        "skipped_empty": 0,
        "skipped_ai": 0,
        "posted": 0
    }

    # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä–æ–≤ (—É–ª—É—á—à–µ–Ω–Ω–∞—è)
    logger.info("\nüì¶ Checking aggregators...")
    for agg in KNOWN_AGGREGATORS:
        if count >= MAX_POSTS_PER_RUN: 
            break
        
        key = f"{agg['owner']}/{agg['repo']}"
        last_known_sha = commits.get(key)
        
        # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü–æ–ª—É—á–∞–µ–º –í–°–ï –Ω–æ–≤—ã–µ –∫–æ–º–º–∏—Ç—ã
        new_commits = get_recent_commits(agg['owner'], agg['repo'], last_known_sha)
        
        if not new_commits:
            logger.info(f"   ‚ÑπÔ∏è {agg['name']}: no new commits")
            continue
        
        logger.info(f"   üÜï {agg['name']}: {len(new_commits)} new commit(s)")
        
        # –ü–æ—Å—Ç–∏–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–π (—á—Ç–æ–±—ã –Ω–µ —Å–ø–∞–º–∏—Ç—å)
        c = new_commits[0]
        
        success = await send_message_safe(
            TARGET_CHANNEL_ID,
            f"üîÑ <b>{agg['name']}</b>\n\n"
            f"‚è∞ {get_freshness(c['date'])}\n"
            f"üìù <code>{html.escape(c['msg'])}</code>\n"
            f"üìä +{len(new_commits)} –∫–æ–º–º–∏—Ç(–æ–≤)\n\n"
            f"üîó <a href='{c['url']}'>–ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å –∫–æ–º–º–∏—Ç</a>"
        )
        
        if success:
            commits[key] = c['sha']
            count += 1
            stats["posted"] += 1
            await asyncio.sleep(MESSAGE_DELAY)

    # 2. –ü–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å–∞–º
    logger.info("\nüîç Searching repositories...")
    for s in FRESH_SEARCHES:
        if count >= MAX_POSTS_PER_RUN: 
            break
        
        if not check_rate_limit():
            logger.warning("‚ö†Ô∏è API limit reached during search. Stopping.")
            break
        
        logger.info(f"\nüîç {s['name']} (priority: {s.get('priority', 5)})...")
        items = search_fresh_repos(s['query'])
        
        stats["total_found"] += len(items)

        if not items:
            continue

        candidates = []
        for i in items:
            repo_id = str(i['id'])
            full_name = i.get('full_name', 'unknown')
            
            if repo_id in posted:
                logger.debug(f"   ‚è≠ Already posted: {full_name}")
                stats["skipped_posted"] += 1
                continue
            
            if not quick_filter(i.get('full_name'), i.get('description'), i.get('stargazers_count', 0)):
                stats["skipped_filter"] += 1
                continue
            
            if is_likely_fork_spam(i):
                logger.debug(f"   ‚è≠ Fork spam: {full_name}")
                stats["skipped_fork"] += 1
                continue
            
            owner, repo = full_name.split('/')
            if is_repo_empty(owner, repo, repo_cache):
                stats["skipped_empty"] += 1
                continue
            
            candidates.append(i)

        logger.info(f"   üìä Candidates after filtering: {len(candidates)}")

        if not candidates:
            continue

        batch_size = 5  # ‚úÖ –ò–ó–ú–ï–ù–ï–ù–û: 5 –≤–º–µ—Å—Ç–æ 4
        for batch_start in range(0, len(candidates), batch_size):
            if count >= MAX_POSTS_PER_RUN: 
                break
            
            batch = candidates[batch_start:batch_start + batch_size]
            
            logger.info(f"   ü§ñ Analyzing batch of {len(batch)} repos...")
            decisions = await analyze_relevance(batch)

            for idx, item in enumerate(batch, 1):
                if count >= MAX_POSTS_PER_RUN: 
                    break
                
                if not decisions.get(idx, False):
                    logger.debug(f"   ‚è≠ AI rejected: {item['full_name']}")
                    stats["skipped_ai"] += 1
                    continue

                final_desc = await generate_desc(item['full_name'], item['description'])

                title = s.get('title', s['name'])
                success = await send_message_safe(
                    TARGET_CHANNEL_ID,
                    build_post(
                        title, 
                        item['full_name'], 
                        item['stargazers_count'],
                        get_freshness(item['pushed_at']), 
                        final_desc, 
                        item['html_url']
                    )
                )
                
                if success:
                    posted.add(str(item['id']))
                    count += 1
                    stats["posted"] += 1
                    logger.info(f"   ‚úÖ Posted: {item['full_name']} (‚≠ê{item['stargazers_count']})")
                    await asyncio.sleep(MESSAGE_DELAY)
            
            await asyncio.sleep(GROQ_DELAY)

    # ‚úÖ –î–û–ë–ê–í–õ–ï–ù–û: –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    logger.info(f"\n{'=' * 60}")
    logger.info("üìä STATISTICS:")
    logger.info(f"   Total found: {stats['total_found']}")
    logger.info(f"   Skipped (already posted): {stats['skipped_posted']}")
    logger.info(f"   Skipped (filter): {stats['skipped_filter']}")
    logger.info(f"   Skipped (fork spam): {stats['skipped_fork']}")
    logger.info(f"   Skipped (empty): {stats['skipped_empty']}")
    logger.info(f"   Skipped (AI): {stats['skipped_ai']}")
    logger.info(f"   ‚úÖ Posted: {stats['posted']}")
    logger.info(f"{'=' * 60}")

    save_state({
        "posted": list(posted)[-3000:],  # ‚úÖ –ò–ó–ú–ï–ù–ï–ù–û: 3000 –≤–º–µ—Å—Ç–æ 2000
        "commits": commits, 
        "repo_cache": repo_cache
    })
    
    logger.info(f"üèÅ Completed! Published: {count}/{MAX_POSTS_PER_RUN}")
    
    await bot.session.close()

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("\n‚è∏ Interrupted by user")
    except Exception as e:
        logger.error(f"‚ùå Fatal error: {e}", exc_info=True)

