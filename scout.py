import os
import json
import asyncio
import requests
import html
import re
import logging
from datetime import datetime, timedelta, timezone
from aiogram import Bot
from aiogram.client.default import DefaultBotProperties
from aiogram.enums import ParseMode
from aiogram.exceptions import TelegramRetryAfter, TelegramForbiddenError
from groq import Groq
import aiohttp

# ============ LOGGING ============

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    handlers=[
        logging.FileHandler('scout_radar.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ============ CONFIG ============

GROQ_API_KEY = os.getenv("GROQ_API_KEY")
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
TARGET_CHANNEL_ID = os.getenv("CHANNEL_ID")
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")

STATE_FILE = "scout_history.json"
CONFIG_SOURCES_FILE = "config_sources.json"

MAX_AGE_DAYS = 3
MAX_POSTS_PER_RUN = 100
GROQ_DELAY = 2
MESSAGE_DELAY = 3
MIN_STARS = 0
MIN_API_CALLS_REMAINING = 50

API_HEADERS = {
    "Authorization": f"Bearer {GITHUB_TOKEN}",
    "Accept": "application/vnd.github.v3+json"
}

bot = Bot(token=TELEGRAM_BOT_TOKEN, default=DefaultBotProperties(parse_mode=ParseMode.HTML))
groq_client = Groq(api_key=GROQ_API_KEY)

# ============ ĞšĞ›Ğ®Ğ§Ğ•Ğ’Ğ«Ğ• ĞŸĞ ĞĞ•ĞšĞ¢Ğ« (ĞºĞ¾Ğ¼Ğ¼Ğ¸Ñ‚Ñ‹ + Ñ€ĞµĞ»Ğ¸Ğ·Ñ‹) ============

TRACKED_PROJECTS = [
    # Zapret Ğ¸ DPI-Ğ¾Ğ±Ñ…Ğ¾Ğ´
    {"owner": "bol-van", "repo": "zapret", "name": "ğŸ›  Zapret (original)", "priority": "high"},
    {"owner": "bol-van", "repo": "zapret2", "name": "ğŸ›  Zapret 2", "priority": "high"},
    {"owner": "ValdikSS", "repo": "GoodbyeDPI", "name": "ğŸ›  GoodbyeDPI", "priority": "high"},
    {"owner": "hufrea", "repo": "byedpi", "name": "ğŸ›  ByeDPI", "priority": "high"},
    {"owner": "xvzc", "repo": "SpoofDPI", "name": "ğŸ›  SpoofDPI", "priority": "high"},

    # VPN Ğ¸ Ğ¿Ñ€Ğ¾ĞºÑĞ¸
    {"owner": "amnezia-vpn", "repo": "amnezia-client", "name": "ğŸ›¡ Amnezia Client", "priority": "high"},
    {"owner": "amnezia-vpn", "repo": "amneziawg-linux-kernel-module", "name": "ğŸ›¡ AmneziaWG Kernel", "priority": "medium"},
    {"owner": "XTLS", "repo": "Xray-core", "name": "âš¡ Xray-core", "priority": "high"},
    {"owner": "SagerNet", "repo": "sing-box", "name": "ğŸ“¦ Sing-Box", "priority": "high"},
    {"owner": "apernet", "repo": "hysteria", "name": "ğŸš€ Hysteria", "priority": "high"},
    {"owner": "Jigsaw-Code", "repo": "outline-server", "name": "ğŸ“¡ Outline Server", "priority": "medium"},
    {"owner": "Jigsaw-Code", "repo": "outline-client", "name": "ğŸ“¡ Outline Client", "priority": "medium"},

    # ĞŸĞ°Ğ½ĞµĞ»Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ
    {"owner": "Gozargah", "repo": "Marzban", "name": "ğŸ› Marzban", "priority": "high"},
    {"owner": "MHSanaei", "repo": "3x-ui", "name": "ğŸ› 3X-UI", "priority": "high"},
    {"owner": "hiddify", "repo": "hiddify-next", "name": "ğŸ› Hiddify Next", "priority": "high"},
    {"owner": "hiddify", "repo": "Hiddify-Manager", "name": "ğŸ› Hiddify Manager", "priority": "medium"},

    # ĞšĞ»Ğ¸ĞµĞ½Ñ‚Ñ‹
    {"owner": "MatsuriDayo", "repo": "nekoray", "name": "ğŸ± Nekoray", "priority": "high"},
    {"owner": "2dust", "repo": "v2rayN", "name": "ğŸ’» V2RayN", "priority": "high"},
    {"owner": "2dust", "repo": "v2rayNG", "name": "ğŸ“± V2RayNG", "priority": "high"},
    {"owner": "metacubex", "repo": "ClashMeta", "name": "âš”ï¸ Clash Meta", "priority": "medium"},
    {"owner": "metacubex", "repo": "mihomo", "name": "âš”ï¸ Mihomo", "priority": "medium"},

    # AntiZapret Ğ¸ ÑĞ¿Ğ¸ÑĞºĞ¸
    {"owner": "AntiZapret", "repo": "antizapret", "name": "ğŸ›¡ AntiZapret", "priority": "high"},
    {"owner": "AntiZapret", "repo": "antizapret-pac-generator-light", "name": "ğŸ›¡ AntiZapret PAC", "priority": "medium"},
    {"owner": "zapret-info", "repo": "z-i", "name": "ğŸ“‹ Zapret-Info", "priority": "medium"},
    {"owner": "C24Be", "repo": "AS_REG", "name": "ğŸ“‹ AS Registry RU", "priority": "medium"},

    # Ğ Ğ¾ÑĞºĞ¾Ğ¼ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ°
    {"owner": "roskomsvoboda", "repo": "censortracker", "name": "ğŸ“¢ CensorTracker", "priority": "high"},
    {"owner": "roskomsvoboda", "repo": "moscow_covid_queues", "name": "ğŸ“¢ RKS Tools", "priority": "low"},
]

# ============ ĞĞ“Ğ Ğ•Ğ“ĞĞ¢ĞĞ Ğ« ĞšĞĞĞ¤Ğ˜Ğ“ĞĞ’ ============

CONFIG_AGGREGATORS = [
    {"owner": "Leon406", "repo": "SubCrawler", "name": "ğŸ“¡ SubCrawler"},
    {"owner": "peasoft", "repo": "NoMoreWalls", "name": "ğŸ“¡ NoMoreWalls"},
    {"owner": "barry-far", "repo": "V2ray-Configs", "name": "ğŸ“¡ V2ray-Configs"},
    {"owner": "mahdibland", "repo": "V2RayAggregator", "name": "ğŸ“¡ V2RayAggregator"},
    {"owner": "Pawdroid", "repo": "Free-servers", "name": "ğŸ“¡ Free-servers"},
    {"owner": "aiboboxx", "repo": "v2rayfree", "name": "ğŸ“¡ V2RayFree"},
]

# ============ ĞŸĞĞ˜Ğ¡ĞšĞĞ’Ğ«Ğ• Ğ—ĞĞŸĞ ĞĞ¡Ğ« ============

FRESH_SEARCHES = [
    {"name": "Zapret Tools", "title": "ğŸ›  Zapret Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹", "query": "zapret OR zapret-discord OR zapret-youtube", "priority": 10},
    {"name": "DPI Bypass", "title": "ğŸ›  DPI Bypass", "query": "dpi-bypass OR bypass-dpi OR nodpi", "priority": 10},
    {"name": "RKN Block", "title": "ğŸ‘ Ğ ĞšĞ Ğ±Ğ»Ğ¾ĞºĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸", "query": "roskomnadzor OR rkn-block OR rkn-bypass", "priority": 10},
    {"name": "TSPU", "title": "ğŸ‘ Ğ¢Ğ¡ĞŸĞ£", "query": "tspu OR sorm OR russia-censorship", "priority": 9},
    {"name": "AntiZapret", "title": "ğŸ›¡ AntiZapret", "query": "antizapret OR anti-zapret", "priority": 10},

    {"name": "Russia VPN Tools", "title": "ğŸ”§ VPN Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ Ğ¤",
     "query": "vpn russia bypass OR vpn russia censorship OR russia vpn tool", "priority": 8},
    {"name": "RU VPN Configs", "title": "ğŸ”§ ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³Ğ¸ VPN Ğ´Ğ»Ñ Ğ Ğ¤",
     "query": "russia vless OR russia reality OR russia hysteria", "priority": 9},

    {"name": "VLESS Reality", "title": "ğŸ”§ VLESS Reality", "query": "vless-reality OR reality-config", "priority": 8},
    {"name": "Hysteria2", "title": "ğŸš€ Hysteria 2", "query": "hysteria2 OR hysteria-2", "priority": 8},
    {"name": "XRay Config", "title": "âš¡ XRay ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³Ğ¸", "query": "xray-config OR xray-russia", "priority": 7},
    {"name": "Amnezia", "title": "ğŸ›¡ Amnezia", "query": "amnezia-vpn OR amneziawg", "priority": 9},
    {"name": "Marzban", "title": "ğŸ› Marzban", "query": "marzban-panel OR marzban-node", "priority": 8},
    {"name": "Geosite RU", "title": "ğŸ—º Geosite Russia", "query": "geosite-russia OR geoip-russia", "priority": 7},
    {"name": "Domain List RU", "title": "ğŸ“‹ Ğ¡Ğ¿Ğ¸ÑĞºĞ¸ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ²", "query": "russia-domains OR ru-blocked-domains", "priority": 7},
    {"name": "Proxy Configs", "title": "ğŸ“¡ ĞŸÑ€Ğ¾ĞºÑĞ¸ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³Ğ¸", "query": "proxy-config-russia OR free-proxy-russia", "priority": 6},
    {"name": "Sing-Box RU", "title": "ğŸ“¦ Sing-Box", "query": "sing-box-russia OR singbox-config", "priority": 7},
    {"name": "Clash Rules", "title": "âš”ï¸ Clash Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ°", "query": "clash-rules-russia OR clash-meta-russia", "priority": 6},
    {"name": "Shadowsocks", "title": "ğŸ” Shadowsocks", "query": "shadowsocks-russia OR ss-config", "priority": 6},
    {"name": "WireGuard RU", "title": "ğŸ”’ WireGuard", "query": "wireguard-russia OR wg-config-russia", "priority": 6},
    {"name": "Outline", "title": "ğŸ“¡ Outline", "query": "outline-russia OR outline-config", "priority": 6},
    {"name": "Censorship", "title": "ğŸŒ ĞĞ½Ñ‚Ğ¸Ñ†ĞµĞ½Ğ·ÑƒÑ€Ğ°", "query": "anti-censorship russia OR internet-freedom russia", "priority": 7},
]

FRESH_SEARCHES.sort(key=lambda x: x.get('priority', 5), reverse=True)

# ============ Ğ”ĞĞŸ. ĞŸĞĞ˜Ğ¡Ğš Ğ”Ğ›Ğ¯ Ğ˜Ğ¡Ğ¢ĞĞ§ĞĞ˜ĞšĞĞ’ ĞšĞĞĞ¤Ğ˜Ğ“ĞĞ’ ============

CONFIG_SEARCH_QUERIES = [
    "v2ray configs free",
    "vless reality subscription",
    "vless reality v2ray",
    "v2ray subscription link",
    "free vless configs",
    "vpn configs russia vless",
]

CONFIG_URL_PATTERNS = [
    r"https://raw\.githubusercontent\.com[^\s\"']+",
    r"https://github\.com[^\s\"']+/raw[^\s\"']*",
    r"https?://[^\s\"']*(?:sub|subscription|clash\.ya?ml|config|proxy)[^\s\"']*",
]

# ============ VALIDATION ============

def validate_env():
    required = {
        "GROQ_API_KEY": GROQ_API_KEY,
        "TELEGRAM_BOT_TOKEN": TELEGRAM_BOT_TOKEN,
        "CHANNEL_ID": TARGET_CHANNEL_ID,
        "GITHUB_TOKEN": GITHUB_TOKEN
    }

    missing = [k for k, v in required.items() if not v]

    if missing:
        logger.error(f"âŒ Missing environment variables: {', '.join(missing)}")
        return False

    logger.info("âœ… All environment variables validated")
    return True

def check_rate_limit():
    try:
        resp = requests.get("https://api.github.com/rate_limit", headers=API_HEADERS, timeout=10)
        if resp.status_code == 200:
            data = resp.json()
            remaining = data['rate']['remaining']
            limit = data['rate']['limit']

            logger.info(f"ğŸ“Š GitHub API: {remaining}/{limit} calls remaining")

            if remaining < MIN_API_CALLS_REMAINING:
                logger.warning(f"âš ï¸ API limit low ({remaining} left)")
                if remaining < 10:
                    return False
            return True
    except Exception as e:
        logger.warning(f"âš ï¸ Could not check rate limit: {e}")
    return True

# ============ HELPERS ============

def has_non_latin(text):
    if not text:
        return False
    patterns = [
        r'[\u4e00-\u9fff\u3040-\u309f\u30a0-\u30ff\uac00-\ud7af]',
        r'[\u0600-\u06ff\u0750-\u077f\uFB50-\uFDFF\uFE70-\uFEFF]',
        r'[\u0e00-\u0e7f\u1780-\u17ff]',
    ]
    return any(re.search(p, text) for p in patterns)

def get_age_hours(date_string):
    try:
        if not date_string:
            return 9999
        dt = datetime.fromisoformat(date_string.replace('Z', '+00:00'))
        return (datetime.now(timezone.utc) - dt).total_seconds() / 3600
    except:
        return 9999

def get_freshness(date_string):
    hours = get_age_hours(date_string)
    if hours < 1: return "ğŸ”¥ Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾"
    elif hours < 6: return f"ğŸ”¥ {int(hours)}Ñ‡ Ğ½Ğ°Ğ·Ğ°Ğ´"
    elif hours < 24: return "ğŸ”¥ Ğ¡ĞµĞ³Ğ¾Ğ´Ğ½Ñ"
    elif hours < 48: return "âœ… Ğ’Ñ‡ĞµÑ€Ğ°"
    elif hours < 72: return "ğŸ“… 2 Ğ´Ğ½Ñ Ğ½Ğ°Ğ·Ğ°Ğ´"
    else: return f"ğŸ“… {int(hours/24)}Ğ´ Ğ½Ğ°Ğ·Ğ°Ğ´"

def is_fresh(date_string):
    return get_age_hours(date_string) <= (MAX_AGE_DAYS * 24)

def safe_desc(desc, max_len=120):
    if desc is None:
        return ""
    desc = str(desc).strip()
    desc = re.sub(r'[ğŸ”¥âš¡ï¸âœ¨ğŸ‰]{3,}', '', desc)
    return desc[:max_len] if desc else ""

def quick_filter(name, desc, stars=0):
    """
    Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğµ Ğ»Ğ¾Ğ²Ğ¸Ñ‚ÑŒ Ğ¼ÑƒÑĞ¾Ñ€ Ñ‚Ğ¸Ğ¿Ğ°
    russian-vocabulary-trainer, steel-market Ğ¸ Ñ‚.Ğ¿.
    """
    text = f"{name} {desc or ''}".lower()
    full_text = f"{name} {desc or ''}"

    if has_non_latin(full_text):
        return False

    if stars < MIN_STARS:
        return False

    # ĞšĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ‚Ğ¾Ğ¿-ÑĞ»Ğ¾Ğ²Ğ° (Ğ½ĞµÑ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ñ‚ĞµĞ¼Ñ‹)
    irrelevant_categories = [
        # Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ / Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ
        'vocabulary', 'trainer', 'learning', 'educational', 'course',
        'tutorial', 'lesson', 'homework', 'student', 'university',
        'language-learning', 'flashcard', 'quiz',

        # Ğ±Ğ¸Ğ·Ğ½ĞµÑ / Ñ€Ñ‹Ğ½Ğ¾Ğº
        'market', 'steel', 'trading', 'business', 'finance',
        'ecommerce', 'shop', 'store', 'retail', 'analytics',

        # Ğ´ĞµĞ¼Ğ¾ / Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹
        'example-', 'demo-', 'template', 'boilerplate', 'starter',
        'practice', 'exercise', 'sample',

        # Ğ¿Ñ€Ğ¾Ñ‡Ğ¸Ğ¹ Ğ¾Ñ„Ñ„Ñ‚Ğ¾Ğ¿
        'recipe', 'cooking', 'food', 'restaurant', 'travel',
        'portfolio', 'resume', 'cv',
        'game', 'minigame',
    ]
    if any(cat in text for cat in irrelevant_categories):
        logger.debug(f"   âŒ Filtered by category: {name}")
        return False

    # Ğ•ÑĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ "russia"/"russian", Ñ‚Ñ€ĞµĞ±ÑƒĞµĞ¼ VPN-ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚
    if 'russia' in text or 'russian' in text:
        vpn_context_required = [
            'vpn', 'proxy', 'bypass', 'dpi', 'censorship',
            'block', 'unblock', 'freedom', 'gfw',
            'zapret', 'rkn', 'sorm', 'tspu',
            'vless', 'vmess', 'xray', 'v2ray', 'reality',
            'shadowsocks', 'trojan', 'hysteria', 'wireguard',
            'amnezia', 'outline', 'clash', 'sing-box',
        ]
        if not any(ctx in text for ctx in vpn_context_required):
            logger.debug(f"   âŒ 'russia' without VPN context: {name}")
            return False

    whitelist = [
        'russia', 'russian', 'ru-', 'roskomnadzor', 'rkn', 'antizapret',
        'zapret', 'tspu', 'sorm', 'amnezia', 'hysteria', 'reality',
        'marzban', 'xray', 'v2ray', 'vless', 'trojan', 'shadowsocks',
        'clash', 'sing-box', 'bypass', 'proxy', 'vpn', 'dpi', 'gfw',
        'censorship', 'freedom', 'unblock'
    ]
    if any(w in text for w in whitelist):
        return True

    blacklist = [
        'china', 'chinese', 'cn-', 'iran', 'persian', 'vietnam',
        'homework', 'tutorial', 'example-', 'template', 'deprecated',
        'test-repo', 'demo-', 'practice', 'learning', 'course'
    ]
    if any(k in text for k in blacklist):
        return False

    return False

def is_likely_fork_spam(item):
    if not item.get('fork'):
        return False
    if item.get('stargazers_count', 0) == 0 and item.get('forks_count', 0) == 0:
        return True
    return False

# ============ GITHUB API FUNCTIONS (ASYNC) ============

async def get_default_branch(session, owner, repo):
    """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ default branch Ñ‡ĞµÑ€ĞµĞ· API"""
    url = f"https://api.github.com/repos/{owner}/{repo}"
    try:
        async with session.get(url) as resp:
            if resp.status == 200:
                data = await resp.json()
                return data.get('default_branch', 'main')
    except Exception as e:
        logger.debug(f"Error getting default branch for {owner}/{repo}: {e}")
    return 'main'

async def fetch_repo_text_async(owner, repo):
    """ĞÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° README Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ²ĞµÑ‚ĞºĞ¸"""
    try:
        async with aiohttp.ClientSession(headers=API_HEADERS) as session:
            # Ğ¡Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ default branch
            branch = await get_default_branch(session, owner, repo)
            
            urls = [
                f"https://raw.githubusercontent.com/{owner}/{repo}/{branch}/README.md",
                f"https://raw.githubusercontent.com/{owner}/{repo}/main/README.md",
                f"https://raw.githubusercontent.com/{owner}/{repo}/master/README.md",
            ]
            
            for url in urls:
                try:
                    async with session.get(url, timeout=aiohttp.ClientTimeout(total=8)) as resp:
                        if resp.status == 200:
                            text = await resp.text()
                            logger.debug(f"   âœ… README loaded from {url}")
                            return text
                except asyncio.TimeoutError:
                    logger.debug(f"   â± Timeout loading {url}")
                    continue
                except Exception as e:
                    logger.debug(f"   âš ï¸ Error loading {url}: {e}")
                    continue
                    
    except Exception as e:
        logger.debug(f"Error fetching README for {owner}/{repo}: {e}")
    
    return ""

def get_latest_release(owner, repo):
    url = f"https://api.github.com/repos/{owner}/{repo}/releases/latest"
    try:
        resp = requests.get(url, headers=API_HEADERS, timeout=10)
        if resp.status_code == 200:
            r = resp.json()
            return {
                "tag": r.get('tag_name', ''),
                "name": r.get('name', r.get('tag_name', '')),
                "date": r.get('published_at', r.get('created_at')),
                "url": r.get('html_url', ''),
                "body": (r.get('body', '') or '')[:300],
                "prerelease": r.get('prerelease', False)
            }
        elif resp.status_code == 404:
            logger.debug(f"   No releases for {owner}/{repo}")
    except Exception as e:
        logger.debug(f"Error getting release for {owner}/{repo}: {e}")
    return None

def get_recent_releases(owner, repo, limit=5):
    url = f"https://api.github.com/repos/{owner}/{repo}/releases?per_page={limit}"
    try:
        resp = requests.get(url, headers=API_HEADERS, timeout=10)
        if resp.status_code == 200:
            releases = []
            for r in resp.json():
                if is_fresh(r.get('published_at', r.get('created_at'))):
                    releases.append({
                        "tag": r.get('tag_name', ''),
                        "name": r.get('name', r.get('tag_name', '')),
                        "date": r.get('published_at', r.get('created_at')),
                        "url": r.get('html_url', ''),
                        "body": (r.get('body', '') or '')[:300],
                        "prerelease": r.get('prerelease', False)
                    })
            return releases
    except Exception as e:
        logger.debug(f"Error getting releases for {owner}/{repo}: {e}")
    return []

def get_last_commit(owner, repo):
    url = f"https://api.github.com/repos/{owner}/{repo}/commits?per_page=1"
    try:
        resp = requests.get(url, headers=API_HEADERS, timeout=10)
        if resp.status_code == 200 and resp.json():
            c = resp.json()[0]
            msg = c['commit']['message'].split('\n')[0][:60]

            if has_non_latin(msg):
                return None

            return {
                "sha": c['sha'][:7],
                "date": c['commit']['committer']['date'],
                "msg": msg,
                "url": c['html_url']
            }
    except Exception as e:
        logger.debug(f"Error getting commit for {owner}/{repo}: {e}")
    return None

def search_fresh_repos(query, per_page=40):
    date_filter = (datetime.now(timezone.utc) - timedelta(days=MAX_AGE_DAYS)).strftime('%Y-%m-%d')

    results = []
    seen_ids = set()

    strategies = [
        f"{query}+pushed:>{date_filter}",
        f"{query}+created:>{date_filter}",
    ]

    for strategy in strategies:
        url = (
            f"https://api.github.com/search/repositories"
            f"?q={strategy}&sort=updated&order=desc&per_page={per_page}"
        )

        try:
            resp = requests.get(url, headers=API_HEADERS, timeout=15)
            if resp.status_code == 200:
                for item in resp.json().get('items', []):
                    if item['id'] not in seen_ids:
                        seen_ids.add(item['id'])
                        if is_fresh(item.get('pushed_at')) or is_fresh(item.get('updated_at')):
                            results.append(item)
            elif resp.status_code == 403:
                logger.warning("âš ï¸ GitHub API rate limit!")
                break
        except Exception as e:
            logger.warning(f"âš ï¸ Search error: {e}")

    return results

# ============ STATE MANAGEMENT ============

def load_state():
    if os.path.exists(STATE_FILE):
        try:
            with open(STATE_FILE, "r", encoding='utf-8') as f:
                data = json.load(f)
                # ĞĞ±Ñ€ĞµĞ·Ğ°ĞµĞ¼ posted ÑÑ€Ğ°Ğ·Ñƒ Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞµ
                data['posted'] = data.get('posted', [])[-3000:]
                logger.info(f"ğŸ“‚ Loaded: {len(data.get('posted', []))} posted, {len(data.get('releases', {}))} releases tracked")
                return data
        except Exception as e:
            logger.warning(f"Could not load state: {e}")
    return {"posted": [], "commits": {}, "releases": {}, "repo_cache": {}, "last_run": None}

def save_state(state):
    state['last_run'] = datetime.now(timezone.utc).isoformat()
    # ĞĞ±Ñ€ĞµĞ·Ğ°ĞµĞ¼ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸
    state['posted'] = state.get('posted', [])[-3000:]
    try:
        with open(STATE_FILE, "w", encoding='utf-8') as f:
            json.dump(state, f, indent=2, ensure_ascii=False)
        logger.info(f"ğŸ’¾ State saved")
    except Exception as e:
        logger.error(f"âŒ Could not save state: {e}")

def load_config_sources():
    if os.path.exists(CONFIG_SOURCES_FILE):
        try:
            with open(CONFIG_SOURCES_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            logger.warning(f"Could not load config_sources: {e}")
    return []

def save_config_sources(sources):
    try:
        with open(CONFIG_SOURCES_FILE, "w", encoding="utf-8") as f:
            json.dump(sources, f, indent=2, ensure_ascii=False)
        logger.info(f"ğŸ’¾ Config sources saved: {len(sources)} urls")
    except Exception as e:
        logger.error(f"âŒ Could not save config_sources: {e}")

# ============ AI FUNCTIONS ============

async def analyze_relevance(repos):
    """ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ Ğ±Ğ°Ñ‚Ñ‡Ğ°Ğ¼Ğ¸"""
    if not repos:
        return {}

    text = "\n".join([
        f"{i+1}. {r['full_name']} | â­{r['stargazers_count']} | {safe_desc(r['description'], 80)}"
        for i, r in enumerate(repos)
    ])

    prompt = f"""ĞÑ‚Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒĞ¹ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ»Ñ ĞºĞ°Ğ½Ğ°Ğ»Ğ° Ğ¿Ñ€Ğ¾ Ğ¾Ğ±Ñ…Ğ¾Ğ´ Ğ±Ğ»Ğ¾ĞºĞ¸Ñ€Ğ¾Ğ²Ğ¾Ğº Ğ² Ğ Ğ¤.

âœ… Ğ ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ñ‚ĞµĞ¼Ñ‹:
- VPN, Ğ¿Ñ€Ğ¾ĞºÑĞ¸, Ñ‚ÑƒĞ½Ğ½ĞµĞ»Ğ¸ (vless, vmess, hysteria, reality)
- ĞĞ±Ñ…Ğ¾Ğ´ DPI/Ğ±Ğ»Ğ¾ĞºĞ¸Ñ€Ğ¾Ğ²Ğ¾Ğº (zapret, ByeDPI, GoodbyeDPI)
- ĞŸĞ°Ğ½ĞµĞ»Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ (Marzban, 3x-ui, Hiddify)
- Ğ¡Ğ¿Ğ¸ÑĞºĞ¸ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ²/IP Ğ´Ğ»Ñ Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ° Ğ ĞšĞ
- Ğ˜Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ° Ñ†ĞµĞ½Ğ·ÑƒÑ€Ñ‹ Ğ² Ğ Ğ¾ÑÑĞ¸Ğ¸

âŒ ĞĞµÑ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ (Ğ²ÑĞµĞ³Ğ´Ğ° SKIP):
- ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºÑƒ (vocabulary, language learning)
- Ğ‘Ğ¸Ğ·Ğ½ĞµÑ/Ñ€Ñ‹Ğ½Ğ¾Ğº (market, steel market, trading, ecommerce)
- ĞŸÑ€Ğ¸Ğ¼ĞµÑ€Ñ‹ ĞºĞ¾Ğ´Ğ°/ÑƒÑ‡ĞµĞ±Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ñ‹ Ğ±ĞµĞ· VPN-Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹
- Ğ˜Ğ³Ñ€Ñ‹, Ğ±Ğ¾Ñ‚Ñ‹, ÑƒÑ‚Ğ¸Ğ»Ğ¸Ñ‚Ñ‹ Ğ±ĞµĞ· Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸ Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ° Ğ±Ğ»Ğ¾ĞºĞ¸Ñ€Ğ¾Ğ²Ğ¾Ğº
- Ğ›ÑĞ±Ñ‹Ğµ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ñ‹ Ñ "russia" Ğ‘Ğ•Ğ— VPN/DPI/Ñ†ĞµĞ½Ğ·ÑƒÑ€Ñ‹-ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°

Ğ ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¸:
{text}

ĞÑ‚Ğ²ĞµÑ‚ÑŒ GOOD Ğ¸Ğ»Ğ¸ SKIP Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾:
1: GOOD/SKIP
2: GOOD/SKIP
..."""

    try:
        resp = groq_client.chat.completions.create(
            model="llama-3.1-8b-instant",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=200,
            temperature=0.1
        )

        res = {}
        for line in resp.choices[0].message.content.split('\n'):
            if ':' in line:
                try:
                    idx, verdict = line.split(':', 1)
                    idx = int(idx.strip().replace('.', ''))
                    res[idx] = 'GOOD' in verdict.upper()
                except:
                    pass
        return res
    except Exception as e:
        logger.warning(f"âš ï¸ AI error: {e}")
        return {i: True for i in range(1, len(repos) + 1)}

async def generate_desc(name, desc):
    if desc and len(desc) > 25 and not has_non_latin(desc):
        return desc

    prompt = f"""Ğ ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¹: {name}
ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ: {desc or 'Ğ½ĞµÑ‚'}

ĞĞ°Ğ¿Ğ¸ÑˆĞ¸ ĞºÑ€Ğ°Ñ‚ĞºĞ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ (1 Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ, Ğ´Ğ¾ 80 ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ²) Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¾Ğ¼.
ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚: VPN, Ğ¾Ğ±Ñ…Ğ¾Ğ´ Ğ±Ğ»Ğ¾ĞºĞ¸Ñ€Ğ¾Ğ²Ğ¾Ğº.

ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ:"""

    try:
        resp = groq_client.chat.completions.create(
            model="llama-3.1-8b-instant",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=60,
            temperature=0.3
        )
        generated = resp.choices[0].message.content.strip()
        if generated and not has_non_latin(generated):
            return generated
    except Exception as e:
        logger.debug(f"Error generating description: {e}")

    return "Ğ˜Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ° Ğ±Ğ»Ğ¾ĞºĞ¸Ñ€Ğ¾Ğ²Ğ¾Ğº"

# Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ñ€ĞµĞ¿Ğ¾ Ğ¿Ğ¾ README (Ñ‚ĞµĞ¿ĞµÑ€ÑŒ async)
async def check_repo_relevance(owner: str, repo: str, repo_cache: dict) -> bool:
    """
    Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ: Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ README Ğ½Ğ° VPN/DPI-ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚,
    Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ¾Ğ²Ğ°Ñ‚ÑŒ vocabulary-trainer, steel-market Ğ¸ Ñ‚.Ğ¿.
    Ğ¡ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ².
    """
    cache_key = f"relevance:{owner}/{repo}"
    
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ĞºÑÑˆ
    if cache_key in repo_cache:
        return repo_cache[cache_key]
    
    text = await fetch_repo_text_async(owner, repo)
    if not text:
        repo_cache[cache_key] = False
        return False

    low = text.lower()

    required_terms = [
        'vpn', 'proxy', 'bypass', 'censorship', 'dpi',
        'vless', 'vmess', 'xray', 'v2ray', 'shadowsocks',
        'trojan', 'hysteria', 'wireguard', 'clash', 'sing-box',
        'zapret', 'rkn', 'roskomnadzor', 'sorm', 'tspu',
    ]
    if not any(term in low for term in required_terms):
        logger.debug(f"   âŒ No VPN/DPI terms in README: {owner}/{repo}")
        repo_cache[cache_key] = False
        return False

    bad_signs = [
        'vocabulary trainer', 'language learning', 'flashcard',
        'steel market', 'commodity market', 'stock market',
        'cooking recipe', 'restaurant', 'shopping cart', 'ecommerce',
    ]
    if any(sign in low for sign in bad_signs):
        logger.debug(f"   âŒ Irrelevant content in README: {owner}/{repo}")
        repo_cache[cache_key] = False
        return False

    repo_cache[cache_key] = True
    return True

# ============ TELEGRAM ============

async def send_message_safe(chat_id, text):
    if has_non_latin(text):
        logger.warning("âš ï¸ Blocked message with hieroglyphs!")
        return False

    for attempt in range(3):
        try:
            await bot.send_message(chat_id, text, disable_web_page_preview=True)
            return True
        except TelegramRetryAfter as e:
            logger.warning(f"âš ï¸ Flood control: waiting {e.retry_after}s")
            await asyncio.sleep(e.retry_after)
        except TelegramForbiddenError:
            logger.error("âŒ Bot blocked by user/chat")
            return False
        except Exception as e:
            logger.warning(f"âš ï¸ Send attempt {attempt+1} failed: {e}")
            await asyncio.sleep(2 ** attempt)
    return False

# ============ POST BUILDERS ============

def build_release_post(project_name, release, owner, repo):
    tag = release['tag']
    name = release['name'] or tag
    body = release['body']

    if body:
        body = re.sub(r'#{1,6}\s*', '', body)
        body = re.sub(r'\*{1,2}([^*]+)\*{1,2}', r'\1', body)
        body = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', body)
        body = body[:200] + ('...' if len(body) > 200 else '')

    text = (
        f"ğŸš€ <b>ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ĞµĞ»Ğ¸Ğ·: {html.escape(project_name)}</b>\n\n"
        f"ğŸ“¦ <code>{owner}/{repo}</code>\n"
        f"ğŸ· Ğ’ĞµÑ€ÑĞ¸Ñ: <b>{html.escape(tag)}</b>\n"
        f"â° {get_freshness(release['date'])}\n"
    )

    if body:
        text += f"\nğŸ“ {html.escape(body)}\n"

    text += f"\nğŸ”— <a href='{release['url']}'>Ğ¡ĞºĞ°Ñ‡Ğ°Ñ‚ÑŒ Ñ€ĞµĞ»Ğ¸Ğ·</a>"

    return text

def build_commit_post(project_name, commit, owner, repo):
    return (
        f"ğŸ”„ <b>{html.escape(project_name)}</b>\n\n"
        f"ğŸ“¦ <code>{owner}/{repo}</code>\n"
        f"â° {get_freshness(commit['date'])}\n"
        f"ğŸ“ <code>{html.escape(commit['msg'])}</code>\n\n"
        f"ğŸ”— <a href='{commit['url']}'>ĞŸĞ¾ÑĞ¼Ğ¾Ñ‚Ñ€ĞµÑ‚ÑŒ ĞºĞ¾Ğ¼Ğ¼Ğ¸Ñ‚</a>"
    )

def build_repo_post(title, repo_full_name, stars, freshness, description, url):
    return (
        f"<b>{title}</b>\n\n"
        f"ğŸ“¦ <code>{html.escape(repo_full_name)}</code>\n"
        f"â­ï¸ {stars} | â° {freshness}\n"
        f"ğŸ’¡ {html.escape(description)}\n\n"
        f"ğŸ”— <a href='{url}'>ĞÑ‚ĞºÑ€Ñ‹Ñ‚ÑŒ Ğ½Ğ° GitHub</a>"
    )

# ============ DISCOVER CONFIG SOURCES ============

def extract_config_urls(text: str):
    urls = set()
    if not text:
        return []

    for pattern in CONFIG_URL_PATTERNS:
        for m in re.findall(pattern, text):
            urls.add(m.strip())

    candidates = []
    for u in urls:
        low = u.lower()
        if any(proto in low for proto in ["vless", "vmess", "hysteria", "trojan", "shadow", "sub", "clash"]):
            candidates.append(u)
    return candidates

def filter_url_for_russia_and_vless(url: str) -> bool:
    low = url.lower()

    if not any(p in low for p in ["vless", "reality", "vmess", "xray", "v2ray", "clash", "sub", "subscription"]):
        return False

    bad_markers = ["iran", "/ir-", "iran-"]
    if any(b in low for b in bad_markers):
        return False

    if re.search(r'Sub\d+\.txt$', url):
        return False

    return True

async def discover_config_sources():
    logger.info("\nğŸŒ Discovering new config sources...")
    existing_sources = set(load_config_sources())
    new_sources = set()

    max_repos = 40
    repos_checked = 0

    for q in CONFIG_SEARCH_QUERIES:
        if repos_checked >= max_repos:
            break

        if not check_rate_limit():
            break

        logger.info(f"   ğŸ” Searching configs for query: {q}")
        items = search_fresh_repos(q, per_page=20)
        if not items:
            continue

        for item in items:
            if repos_checked >= max_repos:
                break

            full_name = item["full_name"]
            owner, repo = full_name.split("/")
            repos_checked += 1

            if not quick_filter(full_name, item.get("description"), item.get("stargazers_count", 0)):
                continue

            text = await fetch_repo_text_async(owner, repo)
            if not text:
                continue

            urls = extract_config_urls(text)
            for u in urls:
                if not filter_url_for_russia_and_vless(u):
                    continue
                if u not in existing_sources:
                    logger.info(f"   ğŸ†• Config source: {u}")
                    new_sources.add(u)

    if new_sources:
        merged = list(existing_sources | new_sources)
        save_config_sources(merged)
    else:
        logger.info("â„¹ï¸ No new config sources found")

# ============ MAIN ============

async def main():
    logger.info("=" * 60)
    logger.info("ğŸ•µï¸  SCOUT RADAR v8.3 (optimized)")
    logger.info("=" * 60)

    if not validate_env():
        return

    if not check_rate_limit():
        logger.error("âŒ Insufficient API calls. Exiting.")
        return

    state = load_state()
    posted = set(state.get("posted", []))
    commits = state.get("commits", {})
    releases = state.get("releases", {})
    repo_cache = state.get("repo_cache", {})
    count = 0

    # 1. Ğ Ğ•Ğ›Ğ˜Ğ—Ğ«
    logger.info("\nğŸš€ Checking releases of tracked projects...")
    for project in TRACKED_PROJECTS:
        if count >= MAX_POSTS_PER_RUN:
            break

        owner = project['owner']
        repo = project['repo']
        key = f"{owner}/{repo}"

        fresh_releases = get_recent_releases(owner, repo)
        if not fresh_releases:
            continue

        for rel in fresh_releases:
            if count >= MAX_POSTS_PER_RUN:
                break

            release_key = f"{key}:{rel['tag']}"
            if release_key in releases:
                continue

            logger.info(f"   ğŸ†• Release: {project['name']} {rel['tag']}")
            success = await send_message_safe(
                TARGET_CHANNEL_ID,
                build_release_post(project['name'], rel, owner, repo)
            )

            if success:
                releases[release_key] = rel['date']
                count += 1
                await asyncio.sleep(MESSAGE_DELAY)

    # 2. ĞšĞĞœĞœĞ˜Ğ¢Ğ«
    logger.info("\nğŸ”„ Checking commits of tracked projects...")
    for project in TRACKED_PROJECTS:
        if count >= MAX_POSTS_PER_RUN:
            break

        if project.get('priority') == 'low' and count > MAX_POSTS_PER_RUN // 2:
            continue

        owner = project['owner']
        repo = project['repo']
        key = f"{owner}/{repo}"

        commit = get_last_commit(owner, repo)
        if not commit:
            continue
        if not is_fresh(commit['date']):
            continue
        if commits.get(key) == commit['sha']:
            continue

        logger.info(f"   ğŸ†• Commit: {project['name']}")
        success = await send_message_safe(
            TARGET_CHANNEL_ID,
            build_commit_post(project['name'], commit, owner, repo)
        )

        if success:
            commits[key] = commit['sha']
            count += 1
            await asyncio.sleep(MESSAGE_DELAY)

    # 3. ĞĞ“Ğ Ğ•Ğ“ĞĞ¢ĞĞ Ğ« ĞšĞĞĞ¤Ğ˜Ğ“ĞĞ’
    logger.info("\nğŸ“¡ Checking config aggregators...")
    for agg in CONFIG_AGGREGATORS:
        if count >= MAX_POSTS_PER_RUN:
            break

        owner = agg['owner']
        repo = agg['repo']
        key = f"{owner}/{repo}"

        commit = get_last_commit(owner, repo)
        if not commit or not is_fresh(commit['date']):
            continue
        if commits.get(key) == commit['sha']:
            continue

        logger.info(f"   ğŸ†• {agg['name']}")
        success = await send_message_safe(
            TARGET_CHANNEL_ID,
            build_commit_post(agg['name'], commit, owner, repo)
        )

        if success:
            commits[key] = commit['sha']
            count += 1
            await asyncio.sleep(MESSAGE_DELAY)

    # 4. ĞŸĞĞ˜Ğ¡Ğš ĞĞĞ’Ğ«Ğ¥ Ğ Ğ•ĞŸĞĞ—Ğ˜Ğ¢ĞĞ Ğ˜Ğ•Ğ’
    logger.info("\nğŸ” Searching for new repositories...")
    for s in FRESH_SEARCHES:
        if count >= MAX_POSTS_PER_RUN:
            break

        if not check_rate_limit():
            break

        logger.info(f"\nğŸ” {s['name']}...")
        items = search_fresh_repos(s['query'])
        if not items:
            continue

        candidates = []
        for i in items:
            repo_id = str(i['id'])

            if repo_id in posted:
                continue
            if not quick_filter(i.get('full_name'), i.get('description'), i.get('stargazers_count', 0)):
                continue
            if is_likely_fork_spam(i):
                continue

            candidates.append(i)

        if not candidates:
            continue

        # ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ±Ğ°Ñ‚Ñ‡Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ 3 Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ
        batch_size = 3
        for batch_start in range(0, len(candidates), batch_size):
            if count >= MAX_POSTS_PER_RUN:
                break

            batch = candidates[batch_start:batch_start + batch_size]
            decisions = await analyze_relevance(batch)

            for idx, item in enumerate(batch, 1):
                if count >= MAX_POSTS_PER_RUN:
                    break

                if not decisions.get(idx, False):
                    logger.debug(f"   â­ AI filtered: {item['full_name']}")
                    continue

                owner, repo = item['full_name'].split('/')
                
                # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· README Ñ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼
                is_relevant = await check_repo_relevance(owner, repo, repo_cache)
                if not is_relevant:
                    logger.info(f"   â­ Skipped (irrelevant README): {item['full_name']}")
                    continue

                final_desc = await generate_desc(item['full_name'], item['description'])

                success = await send_message_safe(
                    TARGET_CHANNEL_ID,
                    build_repo_post(
                        s.get('title', s['name']),
                        item['full_name'],
                        item['stargazers_count'],
                        get_freshness(item['pushed_at']),
                        final_desc,
                        item['html_url']
                    )
                )

                if success:
                    posted.add(str(item['id']))
                    count += 1
                    logger.info(f"   âœ… {item['full_name']}")
                    await asyncio.sleep(MESSAGE_DELAY)

            await asyncio.sleep(GROQ_DELAY)

    # 5. ĞŸĞĞ˜Ğ¡Ğš Ğ˜Ğ¡Ğ¢ĞĞ§ĞĞ˜ĞšĞĞ’ ĞšĞĞĞ¤Ğ˜Ğ“ĞĞ’
    await discover_config_sources()

    # SAVE STATE
    save_state({
        "posted": list(posted),
        "commits": commits,
        "releases": releases,
        "repo_cache": repo_cache
    })

    logger.info(f"\n{'=' * 60}")
    logger.info(f"ğŸ Completed! Published: {count} posts")
    logger.info(f"{'=' * 60}")

    await bot.session.close()

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("\nâ¸ Interrupted by user")
    except Exception as e:
        logger.error(f"âŒ Fatal error: {e}", exc_info=True)
